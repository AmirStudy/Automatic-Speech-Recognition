from keras import Modelfrom keras.initializers import RandomNormalfrom keras.layers import Input, LSTM, CuDNNLSTM, Bidirectional, Dense, ReLU, TimeDistributed, BatchNormalization, Dropoutdef get_model(name, **kwargs):    if name == 'deepspeech-ctc':        return _get_deepspeech_ctc(**kwargs)def _get_deepspeech_ctc(is_gpu, input_dim=26, fc_sizes=[], rnn_sizes=[],                        fc_dropouts=[0, 0, 0], output_dim=36, random_seed=123, stddev=0.05, project=[]):    input_tensor = Input([None, input_dim], name='X')    random = RandomNormal(stddev=stddev, seed=random_seed)    x = input_tensor    for fc_size, fc_dropout in list(zip(fc_sizes, fc_dropouts)):        linear = _get_fc_layer(units=fc_size,                               init=random,                               activation='linear')        x = linear(x)        x = BatchNormalization(axis=-1)(x)        x = ReLU(max_value=20)(x)        if fc_dropout:            x = Dropout(fc_dropout)(x)    for rnn_size, project_size in list(zip(rnn_sizes, project)):        rnn = _get_rnn_layer(is_gpu, units=rnn_size)        x = rnn(x)        if project_size:            linear = _get_fc_layer(units=project_size,                                   init=random,                                   activation='linear')            x = linear(x)            x = BatchNormalization(axis=-1)(x)            x = ReLU(max_value=20)(x)    softmax = _get_fc_layer(units=output_dim,                            init=random,                            activation='softmax')    output_tensor = softmax(x)    model = Model(input_tensor, output_tensor, name='DeepSpeech')    return modeldef _get_fc_layer(init, **kwargs):    params = dict(kernel_initializer=init,                  bias_initializer=init,                  **kwargs)    return TimeDistributed(Dense(**params))def _get_rnn_layer(is_gpu, **kwargs):    params = dict(kernel_initializer='glorot_uniform',                  return_sequences=True,                  return_state=False,                  **kwargs)    if is_gpu:        return Bidirectional(CuDNNLSTM(**params), merge_mode='sum')    else:                                   # Activation functions has to be the same        return Bidirectional(LSTM(**params, activation='tanh', recurrent_activation='sigmoid'), merge_mode='sum')